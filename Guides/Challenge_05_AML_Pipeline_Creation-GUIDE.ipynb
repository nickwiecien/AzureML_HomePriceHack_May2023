{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531940a6-8a59-4f76-b598-bb59075fa84d",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d4b02-7c98-439f-a6ee-18d364e68a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86505112-56d4-4ad3-a138-d125792fb8e5",
   "metadata": {},
   "source": [
    "## Connect to Azure ML Workspace using the AML SDK\n",
    "The code snippet below retrieves a reference to your AML workspace - you can interact directly with resources in your workspace via the SDK, similar to how you can use the Studio UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e3755-7946-45ce-895a-b04a66b3afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96188e9-0216-4226-902b-f69e0dd22f4a",
   "metadata": {},
   "source": [
    "## Create an Experiment\n",
    "Experiments are logical containers of script runs which can hold different metrics and experiments. \n",
    "\n",
    "<b>Hint:</b> if you get stuck on the components below, run a search online for azure ml sdk CLASS_NAME to find relevant docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebf3aa-439d-4aae-b5be-6bcd69d3dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# TO-DO:\n",
    "# Update the experiment_name variable below to 'yourinitials_home_price_model_training_custom_script`\n",
    "# Uncomment and create an Experiment object using the AML SDK\n",
    "\n",
    "experiment_name = \"NWK-PIPELINE\"\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8906f0-eddc-4417-9b19-d914c9efc9cc",
   "metadata": {},
   "source": [
    "## Retrieve a Reference to Compute Cluster\n",
    "Get a pointer to your created AML Compute Cluster (`cpucluster-yourinitials`). You will use this as the compute engine for executing your script run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb02541-7669-46de-92e3-a16908a64541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# TO-DO:\n",
    "# Update the cpu_cluster_name variable below to the name of the cluster you previously created (cpucluster-yourinitials)\n",
    "# Uncomment and retrieve a pointer to your ComputeTarget for cpu_cluster\n",
    "\n",
    "cpu_cluster_name = \"cpucluster-nwk\"\n",
    "cpu_cluster = ComputeTarget(ws, cpu_cluster_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbd42316-3079-4215-86eb-14fa73a64404",
   "metadata": {},
   "source": [
    "## Create and Register an Environment\n",
    "AML environments are reusable software environments that contain dependencies for model training/inferencing operations. These environments can be manually created, packaged into reusable docker containers, and then leveraged time and again for different MLOps activities.\n",
    "\n",
    "Create and register a new environment from the exported conda yaml environment definition (`automl_env.yml`). \n",
    "\n",
    "<i>Hint:</i> The [`Environment` class definition reference](https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)?view=azure-ml-py) showcases multiple ways to construct a new environment, including from a conda specification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fd39c-9e10-4cfa-8040-712dc0a0374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# TO-DO:\n",
    "# Retrieve AzureML-AutoML environment definition to be used in RunConfiguration\n",
    "env_name = 'nwk-automl-env'\n",
    "env = Environment.from_conda_specification(name=env_name, file_path=\"automl_env.yml\")\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559f172-e93e-4d60-9a36-22fbbe899457",
   "metadata": {},
   "source": [
    "## Define Output Datasets\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the `register_on_complete()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467b047-f05e-47f5-972b-9bb1c228f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import DataType\n",
    "column_dictionary = {\n",
    "   'ZN':DataType.to_float(),\n",
    "   'INDUS':DataType.to_float(),\n",
    "   'CHAS':DataType.to_float(),\n",
    "   'NOX':DataType.to_float(), \n",
    "   'RM':DataType.to_float(),\n",
    "   'AGE':DataType.to_float(),\n",
    "   'DIS':DataType.to_float(),\n",
    "   'RAD':DataType.to_float(),\n",
    "   'TAX':DataType.to_float(),\n",
    "   'PTRATIO':DataType.to_float(),\n",
    "   'LSTAT':DataType.to_float(),\n",
    "   'CRIM':DataType.to_float(),\n",
    "   'MEDV':DataType.to_float(),\n",
    "}\n",
    "\n",
    "#TO-DO:\n",
    "# Update names of the OutputFileDatasetConfig and dataset objects below\n",
    "default_ds = ws.get_default_datastore()\n",
    "training_data = OutputFileDatasetConfig(name='NWK_Training_Data', destination=(default_ds, 'NWK_training_data/{run-id}')).read_delimited_files(set_column_types=column_dictionary).register_on_complete(name='NWK_Training_Data')\n",
    "testing_data = OutputFileDatasetConfig(name='NWK_Testing_Data', destination=(default_ds, 'NWK_testing_data/{run-id}')).read_delimited_files(set_column_types=column_dictionary).register_on_complete(name='NWK_Testing_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3771c7-5b61-487b-8ae8-724b77f8eea5",
   "metadata": {},
   "source": [
    "## Define Pipeline Parameters\n",
    "`PipelineParameter` objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Update the pipeline parameters below to include parameters for the following variables:\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `datastore_name` | Name of the datastore you created in Challenge 1 |\n",
    "| `data_path` | Path on the datastore above which includes all of the home price CSV files you uploaded |\n",
    "| `model_name` | Name of the model to be registered in your AML workspace upon completion of the run |\n",
    "| `training_percentage` | Percent of data that should be used for training (0.0 - 1.0) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ae19d-7939-4538-994d-a471eb84cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name = PipelineParameter(name='datastore_name', default_value='nwk_datastore')\n",
    "data_path = PipelineParameter(name='data_path', default_value='07-18-2022_014821_UTC/**')\n",
    "model_name = PipelineParameter(name='model_name', default_value='NWK-ATUOML')\n",
    "training_percentage = PipelineParameter(name='training_percentage', default_value=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c409a-7938-4ac4-8489-85de3ddc4ca1",
   "metadata": {},
   "source": [
    "## Define Pipeline Steps\n",
    "The pipeline below consists of three distinct steps which execute an associated python scripts located in the `./pipeline_script_steps` dir in addition to submitting an AutoML job. First, we call get_data.py and retrieve data from your registered datastore and split into test and train subsets which are subsequently registered. Then, we pass the test and training datasets into an AutoML step that trains a custom model. Finally, the final step executes evaluate_and_register.py which loads both the new model (challenger) and current best model (champion) into code and evaluates the provided test dataset. Based on RMSE, if the challenger model performs better, or no model has been registered to-date, the model is registered in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3dd38-70f0-4238-9059-7633373f5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data from AML-linked datastore\n",
    "# Register tabular datasets (test & train) after retrieval\n",
    "get_data_step = PythonScriptStep(\n",
    "    name='Get Data from Blob Storage',\n",
    "    script_name='get_data-ANSWERS.py',\n",
    "    arguments =['--training_data', training_data,\n",
    "                '--testing_data', testing_data,\n",
    "                '--training_percentage', training_percentage,\n",
    "                '--datastore_name', datastore_name,\n",
    "                '--data_path', data_path\n",
    "               ],\n",
    "    outputs=[training_data, testing_data],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "# TO-DO: UPDATE AUTOML SETTINGS PER THE CONFIGURATION BELOW\n",
    "# Allowed Models: XGBoostRegressor, LightGBM\n",
    "# Experiment Timeout Hours: 0.5\n",
    "# Cross Validation: k-folds with 3 folds\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\" : 10,\n",
    "    \"primary_metric\" : 'normalized_root_mean_squared_error',\n",
    "    \"allowed_models\": ['XGBoostRegressor', 'LightGBM'],\n",
    "    \"experiment_timeout_hours\": 0.5,\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"iterations\": 1\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             path = '.',\n",
    "                             compute_target = cpu_cluster,\n",
    "                             run_configuration = run_config,\n",
    "                             featurization = 'auto',\n",
    "                             training_data = training_data,\n",
    "                             label_column_name = 'MEDV',\n",
    "                             **automl_settings)\n",
    "\n",
    "train_model_step = AutoMLStep(name='Train Model (AutoML)',\n",
    "    automl_config=automl_config,\n",
    "    passthru_automl_config=False,\n",
    "    enable_default_model_output=False,\n",
    "    enable_default_metrics_output=False,\n",
    "    allow_reuse=True)\n",
    "\n",
    "evaluate_and_register_step = PythonScriptStep(\n",
    "    name = 'Evaluate and Register Model',\n",
    "    script_name='evaluate_and_register.py',\n",
    "    inputs=[testing_data.as_input(name='testing_dataset')],\n",
    "    arguments=['--model_name', model_name],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config2\n",
    ")\n",
    "\n",
    "evaluate_and_register_step.run_after(train_model_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8128d-272e-49b2-bc77-a5ec388c921f",
   "metadata": {},
   "source": [
    "## Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. \n",
    "\n",
    "<b>Note:</b> based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc07217-621c-4e0a-a147-73aa040a9b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-DO: Update Pipeline syntax below\n",
    "pipeline = Pipeline(workspace = ws, steps=[get_data_step, train_model_step, evaluate_and_register_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbeb60-0216-4ac4-9186-3a6ee551f236",
   "metadata": {},
   "source": [
    "## Create a Published PipelineEndpoint\n",
    "Once we have created our pipeline we will look to retrain our model periodically as new data becomes available. By publishing our pipeline to a `PipelineEndpoint` we can iterate on our pipeline definition but maintain a consistent REST API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d9a91-f153-4fe3-b4e7-fd6391d17de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "def published_pipeline_to_pipeline_endpoint(\n",
    "    workspace,\n",
    "    published_pipeline,\n",
    "    pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description,\n",
    "):\n",
    "    try:\n",
    "        pipeline_endpoint = PipelineEndpoint.get(\n",
    "            workspace=workspace, name=pipeline_endpoint_name\n",
    "        )\n",
    "        print(\"using existing PipelineEndpoint...\")\n",
    "        pipeline_endpoint.add_default(published_pipeline)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        # create PipelineEndpoint if it doesn't exist\n",
    "        print(\"PipelineEndpoint does not exist, creating one for you...\")\n",
    "        pipeline_endpoint = PipelineEndpoint.publish(\n",
    "            workspace=workspace,\n",
    "            name=pipeline_endpoint_name,\n",
    "            pipeline=published_pipeline,\n",
    "            description=pipeline_endpoint_description\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline_endpoint_name = 'NWK-PipelineEndpoint'\n",
    "pipeline_endpoint_description = 'AutoML Training Pipeline for Home Prices Dataset'\n",
    "\n",
    "published_pipeline = pipeline.publish(name=pipeline_endpoint_name,\n",
    "                                     description=pipeline_endpoint_description,\n",
    "                                     continue_on_step_failure=False)\n",
    "\n",
    "published_pipeline_to_pipeline_endpoint(\n",
    "    workspace=ws,\n",
    "    published_pipeline=published_pipeline,\n",
    "    pipeline_endpoint_name=pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=pipeline_endpoint_description\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3cf493-4dc0-403d-8272-011d34c47372",
   "metadata": {},
   "source": [
    "## Trigger a Pipeline Execution from the Notebook\n",
    "You can create an Experiment (logical collection for runs) and submit a pipeline run directly from this notebook by running the commands below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652f06d-e0af-4fe6-8913-b07a4ce9cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
